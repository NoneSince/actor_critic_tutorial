*   model based/free? model free
*   on/off policy? on policy
*   online/offline? online

--------------------------------------------------------------------------------
TODO1:
env = gym.make("Pendulum-v1", render_mode="rgb_array", g=gravity)
policy = PendulumNNPolicy(state_dim=3, action_dim=1, action_range=2)
NN_mean_eval_rewards = one_step_actor_critic(env, policy, gamma=0.97, learning_rate=0.0005, value_learning_rate=0.0005, n_episodes=1000, plotter=plotter)

--------------------------------------------------------------------------------
TODO2:
from collections import deque
def n_step_actor_critic(env, policy, n=5, learning_rate=0.0002, value_learning_rate=0.0002, gamma=0.97, n_episodes=10000, plotter=None):
    mean_eval_rewards = []
    states = deque(maxlen=n)
    actions = deque(maxlen=n)
    rewards = deque(maxlen=n)
    next_states = deque(maxlen=n)
    dones = deque(maxlen=n)
    for episode in range(n_episodes):
        state, _ = env.reset()
        I = 1
        count = 0
        value_parameters = policy.get_value_parameters_vector()
        policy_parameters = policy.get_parameters_vector()
        terminated, truncated = False, False
        while (not terminated) and (not truncated):
            count += 1
            action, log_likelihood_grad = policy.sample_action(state)
            next_state, reward, terminated, truncated, info = env.step(action)
            # store the current state,action, and reward
            states.append(state)
            action.append(action)
            rewards.append(reward)
            next_states.append(next_state)
            dones.append(1 if terminated else 0)

            if terminated:
                count -= 1
                TD_error = 0
                for i in range(count):
                    TD_error += rewards[i] * pow(gamma, i)
                TD_error += pow(gamma, n) * policy.get_value(states[n - 1]) - policy.get_value(states[0])
                # hint: exacly like the main n-step loop, but just teel count, not teel n.
                value_parameters = value_parameters + value_learning_rate * I * TD_error * policy.grad_value(state)
                policy.set_value_parameters_vector(value_parameters)
                policy_parameters = policy_parameters + learning_rate * I * TD_error * log_likelihood_grad
                policy.set_parameters_vector(policy_parameters)
                states.popleft()
                action.popleft()
                rewards.popleft()
                next_states.popleft()
            # n step loop
            # update just the first state!
            if (count == n):
                count -= 1
                # compute TD error by the formula
                TD_error = 0
                for i in range(n):
                    TD_error += rewards[i] * pow(gamma, i)
                TD_error += pow(gamma, n) * policy.get_value(states[n - 1]) - policy.get_value(states[0])

                # calculate the parameters like in the original function
                value_parameters = value_parameters + value_learning_rate * I * TD_error * policy.grad_value(state)
                policy.set_value_parameters_vector(value_parameters)
                policy_parameters = policy_parameters + learning_rate * I * TD_error * log_likelihood_grad
                policy.set_parameters_vector(policy_parameters)

                # delete the oldest state
                states.popleft()
                action.popleft()
                rewards.popleft()
                next_states.popleft()

            I = I * gamma
            state = next_state

        # evaluate:
        if episode % 5 == 0 or episode == n_episodes - 1:
            mean_reward = evaluate_agent(policy, env, n_episodes=5)
            mean_eval_rewards.append(mean_reward)
            if plotter is None:
                print(f"Episode {episode}: Evaluation mean accumulated reward = {mean_reward}")
            else:
                plotter.update_plot(episode, mean_reward)

    return mean_eval_rewards

# second part:
env = gym.make(game, render_mode="rgb_array", g=gravity)
policy = PendulumVanillaPolicy()
n=5
n_step_vanilla_mean_eval_rewards = n_step_actor_critic(env, policy, n=n, gamma=0.97, learning_rate=0.0005, value_learning_rate=0.0005, n_episodes=n_episodes, plotter=plotter)

--------------------------------------------------------------------------------
TODO3:
def reinforce_with_baseline(env, policy, gamma=0.97, learning_rate=0.0005, value_learning_rate=0.0005, n_episodes=100, plotter=None):
    mean_eval_rewards = []
    for episode in range(n_episodes):
        # collect one episode:
        episode_rewards, actions_log_likelihood_grads, states = collect_episode(env, policy)

        # compute returns:
        episode_returns = compute_returns(episode_rewards, gamma)
        episode_returns = normalize_returns(episode_returns)

        # retrive parameters, and change them in the direction of the gradient:
        policy_parameters = policy.get_parameters_vector()
        value_parameters = policy.get_value_parameters_vector()
        for i in range(len(episode_returns)):
            ret = episode_returns[i]
            log_likelihood_grad = actions_log_likelihood_grads[i]
            state = states[i]

            delta = ret - policy.get_value(state)
            value_parameters = value_parameters + value_learning_rate * delta * policy.grad_value(state)
            policy_parameters = policy_parameters + learning_rate * pow(gamma,i) * delta * log_likelihood_grad
        # update the policy with the new parameters:
        policy.set_parameters_vector(policy_parameters)
        policy.set_value_parameters_vector(value_parameters)

        # for visualization only (not a part of the algorithm, and doesnt affect the parameters):
        if episode % 5 == 0 or episode==n_episodes-1:
            mean_reward = evaluate_agent(policy, env, n_episodes=5)
            mean_eval_rewards.append(mean_reward)
            if plotter is None:
                print(f"Episode {episode}: Evaluation mean accumulated reward = {mean_reward}")
            else:
                plotter.update_plot(episode, mean_reward)

    return mean_eval_rewards

# second part:
env = gym.make(game, render_mode="rgb_array", g=gravity)
policy = PendulumVanillaPolicy()
reinforce_with_baseline_vanilla_mean_eval_rewards = reinforce_with_baseline(env, policy, gamma=0.97, learning_rate=0.0005, value_learning_rate=0.0005, n_episodes=n_episodes, plotter=plotter)
