{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf\n",
    "\n",
    "https://github.com/CLAIR-LAB-TECHNION/FSTMA-course/blob/main/tutorials/tut04/Policy_Gradient.ipynb\n",
    "\n",
    "https://github.com/CLAIR-LAB-TECHNION/FSTMA-course/blob/main/tutorials/tut06/Actor%20Critic.ipynb\n",
    "\n",
    "https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63\n",
    "\n",
    "https://gymnasium.farama.org/environments/classic_control/pendulum/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Environment For This Tuturial\n",
    "*Todo: introduce the environment with some code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# for simplicity and time saving, we will make our environment easier by reducing gravity\n",
    "gravity = 2.0\n",
    "\n",
    "# create one environment for training, and one for rendering:\n",
    "env = gym.make(\"Pendulum-v1\", g=gravity)\n",
    "render_env = gym.make(\"Pendulum-v1\", render_mode=\"human\", g=gravity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran for 200 time steps and accumulated -400.65106764665074 reward\n"
     ]
    }
   ],
   "source": [
    "# run one episode with random actions:\n",
    "terminated, truncated = False, False\n",
    "total_reward = 0.0\n",
    "time_steps = 0\n",
    "observation, info = render_env.reset()\n",
    "while not terminated and not truncated:\n",
    "    action = render_env.observation_space.sample()\n",
    "    observation, reward, terminated, truncated, info = render_env.step(action)\n",
    "    total_reward += reward\n",
    "    time_steps += 1\n",
    "print(f\"Ran for {time_steps} time steps and accumulated {total_reward} reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Policy Based Methods\n",
    "We have seen that Q learning is not scalable to high dimensional state space, thus we use function approximation methods (such as neural networks) to estimate our Q function. But we still have to output the value of each action, what if we have continuous or too many possible actions? we will meet the course of dimensionality.\n",
    "\n",
    "In this tutorial we are going to present a different approach: **Policy Based Methods** and specifically **Policy Gradient Methods**.\n",
    "Instead of learning the Q function, we will learn a **parameterized policy** $\\pi_{\\theta}(s) = \\pi(s;\\theta)$ where $\\theta$ is a set of parameters and $\\pi$ is a distribution function over all possible actions.\n",
    "\n",
    "*TODO: add sketch of value based and policy based methods*\n",
    "\n",
    "\n",
    "TODO: Other motivations such as stochastic policy\n",
    "optional: explain why stochastic policy is better.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parameterized Policy\n",
    "A parametrized policy can be any function with parameters, that has an input of a state, and outputs a distribution function over actions, and it has learnable parameters. One representation of a parametric function is a neural network. If you don't know neural network don't worry about it. All you have to know is that it is a function approximator with learnable parameters we will call $\\theta$.\n",
    "\n",
    "*TODO: add the code that loads a policy and show what it can do (with explanation)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Improving the policy\n",
    "Now that we have a parameterized policy, we need to find the best parameters $\\theta$ that will lead this policy to accumulate as high a reward as possible. How do we do that? possibly with gradient descent.\n",
    "\n",
    "We would like to maximize the return. We will denote the return here with $J$. Note that it depends on the policy, thus it depends on its parameters $\\theta$.\n",
    "\n",
    "![](assets/return.png)\n",
    "\n",
    "If we could compute the gradient of $J$ with respect to the parameters of the policy $\\theta$, we would be able to take a small step in the parameter space in the direction of the gradient, thus increasing the return and improving the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Policy Gradient Theorem\n",
    "It Turns out (and it's not that obvious) that the gradient of the return with respect to the policy parameter can be computed.\n",
    "The proof is not trivial, but the result is\n",
    "\n",
    "![](assets/pg_theorem.png)\n",
    "\n",
    "*TODO : explain it, including why there is an expectation*\n",
    "\n",
    "*TODO: add code example of extracting the grad*\n",
    "\n",
    "*TODO: add proof*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE\n",
    "![](assets/reinforce_paper.png)\n",
    "Policy gradient is just a Theorem or a concept. REINFORCE, is an algorithm that uses the policy gradient theorem, along with monte carlo in order to train a parametrized policy.\n",
    "\n",
    "Recall that we don't know the distribution of $Q^{\\pi_{\\theta}}(s,a)$, thus we can't compute this expectation.\n",
    "In REINFORCE we approximate the expectation over the Q value of pi, with samples of returns from the environment.\n",
    "\n",
    "*TODO: Write a more correct version in power point and add it here*\n",
    "![](assets/REINFORCE.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## REINFORCE Implementation\n",
    "add most of the algorithm, and todo to complete it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Solving Pendulum Environment With Reinforce\n",
    "Run Reinforce and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Drawback of REINFORCE\n",
    "Note the oscillation in the convergence of reinforce, this happens due to the high variance in the monte carlo.\n",
    "One approach to solve this, and get a better algorithm is replacing the monte carlo approximation of the return, with a value function approximate like in Q learning.\n",
    "Such methods are called **Actor-Critic** methods.\n",
    "\n",
    "*TODO: add celll that loads DDPG and trains it on pendulum, then compares it to REINFORCE with TODOS*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}