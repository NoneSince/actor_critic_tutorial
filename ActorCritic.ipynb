{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aZMmMpiymC3f"
      },
      "source": [
        "# Actor Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAZflT_eF2WC",
        "outputId": "b3c5a31b-0f4d-45c9-c83f-800e2c6ce34d"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "!pip install \"git+https://github.com/NoneSince/actor_critic_tutorial\"\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install scipy\n",
        "!pip install torch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gk_e0dM0mC3n"
      },
      "source": [
        "## The environment for this tutorial\n",
        "In this tutorial, *like the policy gradient tutorial*, we are going to work on the gymnasium Pendulum envrionment: https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/pendulum.gif\" style=\"height:200px\">\n",
        "\n",
        "\n",
        "In this envrionment, the agents goal is to level the pendulum so it will face up, and keep it there.\n",
        "\n",
        "The agents obseves 3 dimensional vector of the pendulum free end position (x,y) and it's angular velocity:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/pendulum_state.png\" style=\"height:150px\">\n",
        "\n",
        "It's action is one element, which is what toque (force) to apply on the pendulum.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/pendulum_action.png\" style=\"height:70px\">\n",
        "\n",
        "And the reward function is negative (penalty) for how far the pendulum is from being up, for it's angular velocity, and for the torque the agent applies.\n",
        "The reward in each time step can be between -16.27 to 0.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/pendulum_reward.png\" style=\"height:35px\">\n",
        "\n",
        "At each episode, the pendulum starts at a random state, and the agent plays for a fixed number of steps $T=200$\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0e75hs-rmC3o"
      },
      "source": [
        "---\n",
        "# Reminder from policy gradient tutorial:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IHmgcQbfmC3o"
      },
      "source": [
        "## Policy Gradient Theorem\n",
        "It turns out (and it's not that obvious) that the gradient of the return with respect to the policy parameter can be computed.\n",
        "The proof is not trivial, but the result is\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/pg_theorem.png\" style=\"height:80px\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjbfc_S5mC3p"
      },
      "source": [
        "## Parameterized Policy\n",
        "A parametrized policy can be any function with parameters that has an input of a state, outputs a distribution function over actions, and has learnable parameters.\n",
        "\n",
        "We will use this linear policy parametrization:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/policy_func.jpg\" style=\"height:100px\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ZHIfI_mC3p"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/REINFORCE.png\" style=\"height:500px\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek9XHW4kmC3q"
      },
      "source": [
        "---\n",
        "# Actor–Critic"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7LJTXRjmC3q"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/general_policy_gradient_algorithm.jpg\" style=\"height:400px\">\n",
        "\n",
        "This lets us do more evaluations and approximations besides the policy PI!\n",
        "\n",
        "These evaluated functions can help us choose the step size of \"gradient log likelyhood\", normalize the values, or subtract a factor for our rewards"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "p2dmUN8nmC3r"
      },
      "source": [
        "As an example:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/reinforce_with_baseline.jpg\" style=\"height:400px\">\n",
        "\n",
        "Here we introduce a TD-error-like parameter. we hold a state-value function for that, and since we update the policy in a monti-carlo approach already, we also update the state-value in a monti-carlo approach."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e8POfwErmC3r"
      },
      "source": [
        "REINFORCE with baseline is not considered an actor-critic method because its state-value function is only used as a baseline, not a critic. In other words, a state-evaluation is called a \"critic\" if it uses the estimation of the subsequent states to update the estimation of the current state (aka bootstrapping)\n",
        "\n",
        "With bootstrapping we introduce a bias and an asymptotic dependence on the quality of the function approximation, but this is often beneficial because it reduces the variance."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lMDgD5ISltO9"
      },
      "source": [
        "Actor–Critic reinforcement learning methods are policy search algorithms, where the agent learns to map the state to two outputs:\n",
        "\n",
        "1. Recommended action: A probability value for each action in the action space.\n",
        "In short, this is \"Actor\": the policy\n",
        "\n",
        "2. Estimated rewards in the future: Sum of all rewards it expects to receive in the future.\n",
        "In short, this is \"Critic\": the state-value function V, or action-value function Q\n",
        "\n",
        "in more formal way,\n",
        "* The actor corresponds to a conventional action-selection policy, mapping states to actions in a probabilistic manner.\n",
        "* The critic corresponds to a conventional state-value function, mapping states to expected cumulative future reward.\n",
        "\n",
        "Thus, the critic addresses a problem of prediction, whereas the actor is concerned with control.\n",
        "\n",
        "These problems are separable, but are solved simultaneously to find an optimal policy\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fuhp-31smC3r"
      },
      "source": [
        "The main concept:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/actor_critic_scheme.jpg\" style=\"height:500px\">\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IoazeOf9mC3s"
      },
      "source": [
        "The actor-critic faimly:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/actor_critic_template.jpg\" style=\"height:500px\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hng_vpMBmC3s"
      },
      "source": [
        "Note the difference between REINFORCE-with-baseline and the actor-critic family:\n",
        "the policy and value are being updated as we are going in the episode. we don't need to wait a full episode to start evaluating."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*   model based/free?\n",
        "*   on/off policy?\n",
        "*   online/offline?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z10SiT2lE4rY"
      },
      "source": [
        "Limitations of Actor-Critic Methods\n",
        "* High Variance: The actor-critic algorithm uses the observed reward signal to update the policy and value function. This approach can lead to high variance in the estimates, especially when the reward signal is sparse or noisy.\n",
        "* Slow Convergence: The actor-critic algorithm is a model-free reinforcement learning algorithm, which means that it does not use a model of the environment. This makes it slower to converge compared to model-based methods.\n",
        "* Function Approximation Error: The actor and critic networks in the actor-critic algorithm are typically implemented as neural networks that approximate the policy and value function, respectively. The approximation error in these networks can affect the quality of the learned policy and value function.\n",
        "* Sensitivity to Hyperparameters: The actor-critic algorithm is sensitive to the choice of hyperparameters such as the learning rates for the actor and critic, the discount factor, and the architecture of the neural networks. Choosing the right hyperparameters is important for the success of the algorithm, but it can be difficult in practice.\n",
        "* Non-stationarity: The environment in reinforcement learning is non-stationary, meaning that the transition probabilities and rewards can change over time. This can make it difficult for the actor-critic algorithm to learn the optimal policy, especially if the changes are sudden or large."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ELJDVDQMmC3s"
      },
      "source": [
        "The algorithm we have chosen:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/one_step_actor_critic.jpg\" style=\"height:500px\">\n",
        "\n",
        "note: *I* is literally gamma to the power of t, not some parameter"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "21nR6_YwmC3s"
      },
      "source": [
        "As seen above, we need to approximate the policy and the state-value.\n",
        "\n",
        "We will use this linear policy parametrization:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/policy_func.jpg\" style=\"height:300px\">\n",
        "\n",
        "Why do so for PI? So we can easily compute the gradient using the chain rule and the gradient of a linear function:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/log_grad.png\" style=\"height:300px\">\n",
        "\n",
        "\n",
        "And will use this linear value parametrization:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/value_func.jpg\" style=\"height:200px\">\n",
        "\n",
        "Why do so for V? Just to showcase the simplest case, note that no GRAD(LOG(V)) required.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JTa3mrj_mC3s"
      },
      "source": [
        "## Code Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbwupwGZMEwI"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from pg_tutorial.utils import evaluate_agent, visualize_policy\n",
        "from pg_tutorial.utils import ActivePlotter\n",
        "from pg_tutorial.PendulumNNPolicy import PendulumNNPolicy\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# for simplicity and time saving, we will make our environment easier by reducing gravity\n",
        "game = \"Pendulum-v1\"\n",
        "gravity = 1.0\n",
        "env = gym.make(game, render_mode=\"rgb_array\", g=gravity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s41ZNORNmC3t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class PendulumVanillaPolicy:\n",
        "    def __init__(self):\n",
        "        self.theta_1 = np.random.uniform(-1, 1)\n",
        "        self.theta_2 = np.random.uniform(-1, 1)\n",
        "        self.theta_3 = np.random.uniform(-1, 1)\n",
        "\n",
        "        self.omega_1 = np.random.uniform(-1, 1)\n",
        "        self.omega_2 = np.random.uniform(-1, 1)\n",
        "        self.omega_3 = np.random.uniform(-1, 1)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        # action is deterministic for this policy:\n",
        "        return np.array([self.theta_1 * state[0] + self.theta_2 * state[1] + self.theta_3*state[2]])\n",
        "    def get_value(self, state):\n",
        "        return np.array([self.omega_1 * state[0] + self.omega_2 * state[1] + self.omega_3*state[2]])\n",
        "\n",
        "    def gradient(self, state):\n",
        "        return np.array([state[0], state[1], state[2]])\n",
        "\n",
        "    def grad_log_likelihood(self, state):\n",
        "        # compute grad (log(pi(s)) according to the chain rule:\n",
        "        return (1/self.get_action(state)) * self.gradient(state)\n",
        "    def grad_value(self, state):\n",
        "        return np.array([state[0], state[1], state[2]])\n",
        "\n",
        "    def sample_action(self, state):\n",
        "        # used by our algorithm, returns action and the log likelihood gradient of this action\n",
        "        return self.get_action(state), self.grad_log_likelihood(state)\n",
        "\n",
        "    def get_parameters_vector(self):\n",
        "        return np.array([self.theta_1, self.theta_2, self.theta_3])\n",
        "    def get_value_parameters_vector(self):\n",
        "        return np.array([self.omega_1, self.omega_2, self.omega_3])\n",
        "\n",
        "    def set_parameters_vector(self, parameters_vector):\n",
        "        self.theta_1, self.theta_2, self.theta_3 = parameters_vector\n",
        "    def set_value_parameters_vector(self, value_parameters_vector):\n",
        "        self.omega_1, self.omega_2, self.omega_3 = value_parameters_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toGX0w-NmC3u"
      },
      "outputs": [],
      "source": [
        "def one_step_actor_critic(env, policy, learning_rate=0.0002, value_learning_rate=0.0002, gamma=0.97, n_episodes=10000, plotter=None):\n",
        "    mean_eval_rewards = []\n",
        "    for episode in range(n_episodes):\n",
        "        state, _ = env.reset()\n",
        "        I = 1\n",
        "        value_parameters = policy.get_value_parameters_vector()\n",
        "        policy_parameters = policy.get_parameters_vector()\n",
        "        terminated, truncated = False, False\n",
        "        while (not terminated) and (not truncated):\n",
        "            action, log_likelihood_grad = policy.sample_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            if terminated:\n",
        "                TD_error = reward - policy.get_value(state)\n",
        "            else:\n",
        "                TD_error = reward + gamma*policy.get_value(next_state) - policy.get_value(state)\n",
        "\n",
        "            value_parameters = value_parameters + value_learning_rate * I * TD_error * policy.grad_value(state)\n",
        "            policy.set_value_parameters_vector(value_parameters)\n",
        "\n",
        "            policy_parameters = policy_parameters + learning_rate * I * TD_error * log_likelihood_grad\n",
        "            policy.set_parameters_vector(policy_parameters)\n",
        "\n",
        "            I = I*gamma\n",
        "            state = next_state\n",
        "\n",
        "        # for visualization only (not a part of the algorithm, and doesnt affect the parameters):\n",
        "        if episode % 5 == 0 or episode==n_episodes-1:\n",
        "            # add the agent's stats to a grapth or log them\n",
        "            mean_reward = evaluate_agent(policy, env, n_episodes=5)\n",
        "            mean_eval_rewards.append(mean_reward)\n",
        "            if plotter is None:\n",
        "                print(f\"Episode {episode}: Evaluation mean accumulated reward = {mean_reward}\")\n",
        "            else:\n",
        "                plotter.update_plot(episode, mean_reward)\n",
        "\n",
        "    return mean_eval_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A015NRoImC3u"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "n_episodes=1000\n",
        "\n",
        "env = gym.make(game, render_mode=\"rgb_array\", g=gravity)\n",
        "plotter = ActivePlotter(max_iteration=n_episodes,reward_range=(-1900, 0))\n",
        "policy = PendulumVanillaPolicy()\n",
        "vanilla_mean_eval_rewards = one_step_actor_critic(env, policy, gamma=0.97, learning_rate=0.0005, value_learning_rate=0.0005, n_episodes=n_episodes, plotter=plotter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_evaluation = evaluate_agent(policy, env, n_episodes=1000) # way better than after only 100 episodes\n",
        "print(f\"agent evaluation mean reward:\", agent_evaluation)\n",
        "\n",
        "theta_params = policy.get_parameters_vector()\n",
        "print(f\"there are {len(theta_params)} parameters in this policy function\")\n",
        "print(theta_params)\n",
        "omega_params = policy.get_value_parameters_vector()\n",
        "print(f\"there are {len(omega_params)} parameters in this state-value function\")\n",
        "print(omega_params)\n",
        "visualize_policy(policy, env, n_episodes=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2HmzbOBXmC3v"
      },
      "source": [
        "# TODO 1:\n",
        "\n",
        "We provided you with another agent that approximates policy and state-value with a neural network, named *PendulumNNPolicy*\n",
        "\n",
        "Run the new agent,for 1000 episode, and compare with the simple linear agent, with a discount factor of 0.97, and learning rates are 0.0005\n",
        "\n",
        "the new agent needs from you the state dimension, the action dimension, and the max action (regaring the actions, it doesnt take min and max because it supposes  [0 to +m] U [-m to 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKjyoqTmmC3v"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "plotter = ActivePlotter(max_iteration=n_episodes,reward_range=(-1900, 0))\n",
        "\n",
        "### TODO: run ac with NN policy for 1000 episodes ###\n",
        "# create \"Pendulum-v1\" env, and  NN policy:\n",
        "# state_dim= ... # length of the state vector/scalar\n",
        "# action_dim= ... # length of the actiion vector/scalar\n",
        "# action_range= ... # the action values are from (0 to +range) or (0 to -range)\n",
        "# NN_mean_eval_rewards = ... one_step_actor_critic() # uncomment and complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_evaluation = evaluate_agent(policy, env, n_episodes=1000)\n",
        "print(f\"agent evaluation mean reward:\", agent_evaluation)\n",
        "\n",
        "theta_params = policy.get_parameters_vector()\n",
        "print(f\"there are {len(theta_params)} parameters in this policy function\")\n",
        "print(theta_params)\n",
        "omega_params = policy.get_value_parameters_vector()\n",
        "print(f\"there are {len(omega_params)} parameters in this state-value function\")\n",
        "print(omega_params)\n",
        "visualize_policy(policy, env, n_episodes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5krf2ZZmC3v"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(NN_mean_eval_rewards, label=\"NN\")\n",
        "plt.plot(vanilla_mean_eval_rewards, label=\"Vanilla\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wdgY5Vw6JRwu"
      },
      "source": [
        "# TODO 2:\n",
        "As we know, there are two paradigms of updating the agent:\n",
        "\n",
        "Online Learning/TD Learning: Updating the agent after every action taken\n",
        "\n",
        "Monte Carlo: Updating the agent after an episode ends\n",
        "\n",
        "Turns out, N step learning is the general case of these two: the agent takes N steps forwards, then estimates the value of the stated N steps back, based on the N rewards we has collected.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/n_step_idea.jpg\" style=\"height:500px\">\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IPgbIBC-mC3v"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/n_step_totals.jpg\" style=\"height:500px\">\n",
        "\n",
        "For some natural number N, the reward updates looks like this:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/n_step_general_formula.jpg\" style=\"height:200px\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lx3HpPFmC3w"
      },
      "source": [
        "N step learning is very importent in actor critic, because it makes the algorithm more stable and lower the variance.\n",
        "\n",
        "so now, you gonna implement n-step learning by yourself. write the missing parts in this function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06HiOZEAVEAh"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "def n_step_actor_critic(env, policy, n=5, learning_rate=0.0002, value_learning_rate=0.0002, gamma=0.97, n_episodes=10000, plotter=None):\n",
        "    mean_eval_rewards = []\n",
        "    states=deque(maxlen=n)\n",
        "    actions=deque(maxlen=n)\n",
        "    rewards=deque(maxlen=n)\n",
        "    next_states=deque(maxlen=n)\n",
        "    dones=deque(maxlen=n)\n",
        "    for episode in range(n_episodes):\n",
        "        state, _ = env.reset()\n",
        "        I = 1\n",
        "        count=0\n",
        "        value_parameters = policy.get_value_parameters_vector()\n",
        "        policy_parameters = policy.get_parameters_vector()\n",
        "        terminated, truncated = False, False\n",
        "        while (not terminated) and (not truncated):\n",
        "            count+=1\n",
        "            action, log_likelihood_grad = policy.sample_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            #store the current state,action, and reward\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            next_states.append(next_state)\n",
        "            dones.append(1 if terminated else 0)\n",
        "\n",
        "            if terminated:\n",
        "                count-=1\n",
        "                TD_error=0\n",
        "\n",
        "                #hint: exacly like the main n-step loop, but just teel count, not teel n.\n",
        "\n",
        "\n",
        "                #\n",
        "                value_parameters = value_parameters + value_learning_rate * I * TD_error * policy.grad_value(state)\n",
        "                policy.set_value_parameters_vector(value_parameters)\n",
        "                policy_parameters = policy_parameters + learning_rate * I * TD_error * log_likelihood_grad\n",
        "                policy.set_parameters_vector(policy_parameters)\n",
        "                states.popleft()\n",
        "                actions.popleft()\n",
        "                rewards.popleft()\n",
        "                next_states.popleft()\n",
        "            #n step loop\n",
        "            #update just the first state!\n",
        "            if(count==n):\n",
        "              count-=1\n",
        "              #compute TD error by the formula\n",
        "              TD_error=0\n",
        "\n",
        "\n",
        "              #calculate the parameters like in the original function\n",
        "\n",
        "\n",
        "              #delete the oldest state\n",
        "              states.popleft()\n",
        "              actions.popleft()\n",
        "              rewards.popleft()\n",
        "              next_states.popleft()\n",
        "\n",
        "            I = I*gamma\n",
        "            state = next_state\n",
        "\n",
        "        # for visualization only (not a part of the algorithm, and doesnt affect the parameters):\n",
        "        if episode % 5 == 0 or episode==n_episodes-1:\n",
        "            mean_reward = evaluate_agent(policy, env, n_episodes=5)\n",
        "            mean_eval_rewards.append(mean_reward)\n",
        "            if plotter is None:\n",
        "                print(f\"Episode {episode}: Evaluation mean accumulated reward = {mean_reward}\")\n",
        "            else:\n",
        "                plotter.update_plot(episode, mean_reward)\n",
        "\n",
        "    return mean_eval_rewards"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the algorithm for n=5 with the simple vanilla agent and plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "n_episodes=1000\n",
        "plotter = ActivePlotter(max_iteration=n_episodes,reward_range=(-1900, 0))\n",
        "\n",
        "# env = ...\n",
        "# policy = ...\n",
        "n=5\n",
        "# vanilla_mean_eval_rewards = n_step_actor_critic(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_evaluation = evaluate_agent(policy, env, n_episodes=100)\n",
        "print(f\"agent evaluation mean reward:\", agent_evaluation)\n",
        "\n",
        "theta_params = policy.get_parameters_vector()\n",
        "print(f\"there are {len(theta_params)} parameters in this policy function\")\n",
        "print(theta_params)\n",
        "omega_params = policy.get_value_parameters_vector()\n",
        "print(f\"there are {len(omega_params)} parameters in this state-value function\")\n",
        "print(omega_params)\n",
        "visualize_policy(policy, env, n_episodes=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IXrppCqymC3w"
      },
      "source": [
        "# TODO 3 - optional:\n",
        "To remind you, there was an algorithm in-between:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NoneSince/actor_critic_tutorial/master/assets/reinforce_with_baseline.jpg\" style=\"height:400px\">\n",
        "\n",
        "We want you to implement the algorithm, so we give you the REINFORCE to start from, along with some helper functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54zsOrsAmC3w"
      },
      "outputs": [],
      "source": [
        "def collect_episode(env, policy):\n",
        "    \"\"\"\n",
        "    collect one episode with the required data for reinforce, for each step return:\n",
        "     - the reward\n",
        "     - the gradient of the log likelihood for the action taken at the state\n",
        "    each in a list in the length of the episode (we don't need more than that for reinforce)\n",
        "    \"\"\"\n",
        "    episode_rewards, actions_log_likelihood_grads, states = [], [], []  # to be returned\n",
        "    terminated, truncated = False, False\n",
        "    state, _ = env.reset()\n",
        "    while (not terminated) and (not truncated):\n",
        "        action, log_likelihood_grad = policy.sample_action(state)\n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        episode_rewards.append(reward)\n",
        "        actions_log_likelihood_grads.append(log_likelihood_grad)\n",
        "        states.append(state)\n",
        "\n",
        "    return episode_rewards, actions_log_likelihood_grads, states\n",
        "\n",
        "def compute_returns(episode_rewards, gamma):\n",
        "    \"\"\" given reward collected for each step and a discount factor, compute the return for this step, should be efficient. \"\"\"\n",
        "    episode_returns = []\n",
        "    current_return = 0\n",
        "    for reward in episode_rewards[::-1]:\n",
        "        current_return = reward + gamma * current_return\n",
        "        episode_returns.insert(0, current_return)\n",
        "\n",
        "    return episode_returns\n",
        "\n",
        "def normalize_returns(episode_returns):\n",
        "    \"\"\" normalize the returns for an episode for numerical stability \"\"\"\n",
        "    episode_returns = np.array(episode_returns)\n",
        "    episode_returns = (episode_returns - episode_returns.mean()) / (episode_returns.std() + 1e-9)\n",
        "    return episode_returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAuh6KhjmC3w"
      },
      "outputs": [],
      "source": [
        "def reinforce(env, policy, gamma=0.97, learning_rate=0.0005, n_episodes=100, plotter=None):\n",
        "    mean_eval_rewards = []\n",
        "    for episode in range(n_episodes):\n",
        "        # collect one episode:\n",
        "        episode_rewards, actions_log_likelihood_grads, _ = collect_episode(env, policy)\n",
        "\n",
        "        # compute returns:\n",
        "        episode_returns = compute_returns(episode_rewards, gamma)\n",
        "        episode_returns = normalize_returns(episode_returns)\n",
        "\n",
        "        # retrive parameters, and change them in the direction of the gradient:\n",
        "        policy_parameters = policy.get_parameters_vector()\n",
        "        for i in range(min(len(episode_returns), 150)):  # ignore the last 50 steps. Think why?\n",
        "            ret = episode_returns[i]\n",
        "            log_likelihood_grad = actions_log_likelihood_grads[i]\n",
        "\n",
        "            policy_parameters = policy_parameters + learning_rate * ret * log_likelihood_grad\n",
        "        # update the policy with the new parameters:\n",
        "        policy.set_parameters_vector(policy_parameters)\n",
        "\n",
        "        # for visualization only (not a part of the algorithm, and doesnt affect the parameters):\n",
        "        if episode % 5 == 0 or episode==n_episodes-1:\n",
        "            mean_reward = evaluate_agent(policy, env, n_episodes=5)\n",
        "            mean_eval_rewards.append(mean_reward)\n",
        "            if plotter is None:\n",
        "                print(f\"Episode {episode}: Evaluation mean accumulated reward = {mean_reward}\")\n",
        "            else:\n",
        "                plotter.update_plot(episode, mean_reward)\n",
        "\n",
        "    return mean_eval_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap2SlIM9mC3x"
      },
      "outputs": [],
      "source": [
        "def reinforce_with_baseline(env, policy, gamma=0.97, learning_rate=0.0005, value_learning_rate=0.0005, n_episodes=100, plotter=None):\n",
        "    pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QKLfGbzAmC3x"
      },
      "source": [
        "Run the algorithm with the simple vanilla agent and plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl63oAeJmC3x"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "n_episodes=1000\n",
        "plotter = ActivePlotter(max_iteration=n_episodes,reward_range=(-1900, 0))\n",
        "\n",
        "# env = ...\n",
        "# policy = ...\n",
        "# reinforce_with_baseline_vanilla_mean_eval_rewards = reinforce_with_baseline(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrhfEO5TmC3x"
      },
      "outputs": [],
      "source": [
        "agent_evaluation = evaluate_agent(policy, env, n_episodes=100)\n",
        "print(f\"agent evaluation mean reward:\", agent_evaluation)\n",
        "\n",
        "theta_params = policy.get_parameters_vector()\n",
        "print(f\"there are {len(theta_params)} parameters in this policy function\")\n",
        "print(theta_params)\n",
        "omega_params = policy.get_value_parameters_vector()\n",
        "print(f\"there are {len(omega_params)} parameters in this state-value function\")\n",
        "print(omega_params)\n",
        "visualize_policy(policy, env, n_episodes=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
